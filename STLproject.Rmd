---
title: "Implicit Motor Adaptation in Single-Trial Learning"
author: "Marius 't Hart"
date: "2024-10-20"
output:
  html_document: default
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



Implicit single trial learning rates are:

- sensitive to task errors
- capped at a maximum of around 6°
- somewhat predictive of prolonged adaptation
- decreased with terminal feedback, but 
- do not further decrease with delayed feedback

# Overview

In this project we ask two main questions about implicit adaptation using a single-trial learning task, and verify whether or not performance in single-trial learning can generalize to regular prolonged learning paradigms.

**First**: whether or not task errors drive implicit motor adaptation is still a matter of debate, with some papers showing task error only affecting explicit learning [citations], and other showing it also affects implicit adaptation [citations]. Aftereffects in single-trial learning paradigms are supposed to gauge implicit responses only, so here we test if task error affects learning in a single-trial learning paradigm.

**Second**: some papers show that the extent of implicit adaptation reverts back to or towards zero for larger errors [citations], while some other papers show it remains at some capped level [citations]. Here we ask what happens to the _rate_ of implicit adaptation across error sizes. Is rate an appropriate way to express the time course of learning, or is it better described in absolute change per trial?

**Bonus**: single-trial learning paradigms have been used in some studies to answer questions about our motor learning apparatus. However, it has not been tested to what degree performance in single-trial learning paradigms can actually tell us about performance in more standard situations. So, after the single-trial learning phase in the experiment (meant to answer the above two questions) we also include a regular prolonged adaptation task with a relatively low perturbation (20°) in order to evoke implicit adaptation only (Modchalingam et al., 2019). We will then test how well data from the single-trial learning phase predicts adaptation in the prolonged learning phase.

# Setup

First, we'll load all the scripts and functions that we need to run the analyses.

Some of the code requires Marius' personal package to be installed. Find it here, with installation instructions: [https://github.com/thartbm/Reach](https://github.com/thartbm/Reach)

```{r}
# this has functions for downloading, pre-processing and accessing the data:
source('R/data.R')

# this has functions for the attribution-based function:
source('R/attributionModel.R')

# this has functions for the capped implicit function:
source('R/cappedModel.R')

# long exposure fits:
source('R/longExp.R')

# this has functions to do statistics on our data:
source('R/statistics.R')

# here is an approach to do test-retest reliability:
source('R/testRetest.R')

# this file has functions that create figures:
source('R/figures.R')

# this has functions to preprocess the feedback-delay data:
source('R/feedbackDelay.R')

# this has functions to do test/retest comparisons on fit parameters:
source('R/testRetest.R')

```



# Download the data

```{r eval=FALSE, include=FALSE}
# it's not necessary to run this code all the time, so eval=FALSE stops it from running
# include=FALSE also stops it from being shown in notebook output
# change either to get different effects

# This downloads the data from the OSF repository:
downloadData()
# The function has 2 arguments which determines which data gets downloaded, these
# are the default values:
# groups=c('45','60','90','delayed'),
# get=c('demographics', 'processed')

# There are no other groups to get data from. However you could also get the full
# reach trajectories, by adding 'full' to the get argument. Or just a summary 
# performance per participant, that we used to pre-process the data by adding 'short'
# to the get argument.

# If  one of the arguments is empty, no data is downloaded.
# This also means that the rest of this interactive notebook no longer works.

# Note: the interactive version of the R Notebook is the file 'STLproject.Rmd'
# that can be downloaded from the linked GitHub repository. The OSF repository 
# only has "knitted" versions of the notebook: PDF, Word or HTML documents.
```

# Pre-processing of the data

```{r eval=FALSE, include=FALSE}
# This creates STL based files: the difference in no-cursor reach directions in the trials before and after each perturbed trial, with the rotation size, trial number and participant indicated. There are 4 files, two each for groups doing rotations up to 45 degrees and up to 60 degrees, and for those, one file each for point targets and arc targets.
Preprocess_all_files()

# Then we fit the two models to the point and arc target data for each of the participants. This step in particular might take a long time, as it fits 2 models each to the data of 74 participants.
bothModelFits()

# For the prediction part, we want to fit an exponential curve to each participants' long exposure task data. This also takes long, but probably not as long as the other fits.
getLongExpFits()

# Finally, for the dleayed feedback group, we also process the data:
processFeedbackDelayData() 
# This gets the median, normalized reach aftereffect for every participant + rotation + delay

# this fits the two functions to the first 2 and last 2 superblocks for each
# participant in the 45/60/90 group
testRetestFits()
```


# Task error in implicit adaptation

Let's first have a look at the data.

We have taken the difference between the post- and pre-perturbation no-cursor trials, removed any that exceeded 60 degrees in either direction, collapsed response across positive and negative rotations, and then within each participant took the median reach deviation. We also bootstrap a 95% confidence interval of the mean across participants' median reach deviations. (These should be related to t-tests: if the average in one sample falls outside the 95% confidence interval of the other sample, it was unlikely drawn from the same source.)


```{r fig.width=5,fig.height=5.5}
fig23_data()
```
We can see that in the arc targets, where task error is removed (or very much decreased) the reach aftereffects are also much smaller. Or in other words, it looks like the inclusion of task error in the point targets, does facilitate implicit adaptation.

Let's however do the statistics. First, we do repeated measures ANOVAs on the reach aftereffects in both groups. These two ANOVAs have two factors: target type (point or arc) and rotation size (1-45 or 1-60 with 10 values each). Looking at the raw data, there should be an effect of, and of rotation size. There might also be an interaction between target type and rotation as the absolute effect of task error seems larger for the larger rotations (although it might be proportionally the same).

```{r}
taskErrorANOVAs()
```

In all three groups we see a main effect of target type, as well as a main effect of rotation, but no interaction.

# Delayed Feedback

Several papers claim that delaying feedback eliminates implicit adaptation, but we have previously found that it only decreases it. Here we test this again, across a range of rotation sizes and delays, in a within-subject design. In this task, each participant experienced 5 different feedback delays (0, 200, 400, 800 and 1600 ms) for 6 rotation sizes (5, 10, 15, 25, 35 and 45 degrees) with only point targets. The initial implicit reach aftereffects can be seen in figure 2.

The 45 group experienced the same range of rotations, but with continuous feedback, so it serves as a reasonable comparison group. We can see that the aftereffects are much reduced for all delays, including the 0 ms delay, which is essentially terminal feedback condition. It does not look like there are any further decreases with longer delays however.

But first, are there any reach aftereffects after only 1 trial with terminal feedback? We do a t-test versus 0 for the 0 ms delay condition:

```{r}
terminalFeedbackTtest()
```

Yes, we do get implicit reach aftereffects after a single terminal feedback trial (t(49)=6.384, p<.001). The 95% CI (assuming a t-distribution) is 1.04 to 1.99 degrees, with an average amount of implicit adaptation of about 1.5 degrees. That is much less than the roughly 6 degrees we see in the other conditions (a quarter), but also definitely not 0.

Let's test any effects of delays with an ANOVA:

```{r}
delayedFeedbackANOVA()
```

So we can see that terminal feedback decreases initial implicit adaptation but does not eliminate it, and that further delays have no effect on initial implicit adaptation.

# Reach deviations as a function of rotation size

There are broadly two ways of thinking about how implicit adaptation relates to rotation size. First, there is the idea that as errors get very large, it is increasingly less likely that we caused them and hence it also makes less sense for us to automatically correct for them (Wei & Koerding; Tsay??). This would predict lower implicit learning rates for larger rotations. Second, there is the idea that the implicit adaptation systems will do their work no matter what, but they will only learn so much on each trial, i.e. implicit adaptation is "capped" (Kim / Morehead). This would predict that implicit learning rates would increase up to a point with larger and larger errors but it would be capped beyond that point.

These ideas apply to implicit adaptation after a prolonged exposure to a rotation, while here we test initial implicit adaptation instead. Nevertheless, we test the same two ideas, just applied to our single-trial learning data. Below we test two simple models, capturing these two ideas.

## Attenuation model

The error attribution model, uses a Gaussian distribution centred on 0 to capture the idea that larger errors are less likely to be attributed to ourselves, as well as a fraction of the error that would be corrected for if they were completely attributed to ourselves:

$x(p) = p \cdot s \cdot \mathcal{N}(p,w^2)$

Where $p$ denotes the perturbation size in degrees, $s$ the fraction of an error that would be corrected for when it is completely attributed to ourselves, and $w^2$ the width of the normal distribution, and $\mathcal{N}$ denotes a normalized Gaussian probability density function with a mean of 0:

$\mathcal{N}(p,w) = e^{-\frac{p^2}{2w^2}}$

This probability distribution function has a mean of 0 ($p=0$), expressing the idea that when errors are small we fully attribute them to ourselves. It is also normalized to be 1 at that center, such that responses to the rotations close to zero are equal to the $s$ parameter. This makes the fitted parameter values more readable, and values of $s$ can be compared directly, but does not affect the fit itself.

The function has two free parameters: $s$ and $w$, and can be used to predict $x$: the reach deviations evoked by a range of perturbation sizes $p$.

## Capped rate model

The capped rate model uses a cap level, and a fraction of that errors would be corrected for until that cap is reached:

$x(p) = min(c, r \cdot p)$

Where $p$ denotes the perturbation size in degrees, $c$ the maximum level that implicit adaptation can correct for on the first trial, and $r$ the fraction (or rate) of errors that is corrected until $c$ is reached.

The function has two free parameters as well, and can also be used to predict $x$: the reach deviations evoke by a range of perturbation sizes $p$.

## Parameter limits

When fitting the models, we can set a reasonable range of values. For both the rate and slope parameters (of the capped fixed rate function and the attenuation function respectively), we assumed they would be well below 100% learning from any error, but also above 0% learning. So for these parameters, we set the limits to [0.05, 1.0] for the capped fixed-rate function's rate parameter, and to [0.001, 1.0] for the attenuation function's slope parameter.These are comparable but slightly more restrictive for the capped-fixed rate function.

The cap parameter of the capped fixed-rate function, sets a maximum on how much participants learn implicitly after a single trial. The contribution of implicit adaptation after many trials has been shown to consistently be between 10 and 20 degrees. In D'Amario et al, 2024 (bioRxiv) we consistently see around 2 to 3 degrees of learning after 1 trial. To allow for the possibility that people somehow learn the maximum amount after a single trial, while also allowing a little less learning than in our own previous study, we set the limits for this parameter to [1, 25] in degrees.

The width parameter of the attenuation function is harder to set limits for. In Zhang et al, 2024 (eLife) we see maximum learning (and a maximum learning rate) for rotations around 16 degrees. In Tsay et al (which one was it?) we see maximum learning for rotations around 18 degrees. We wanted to give a bit more leeway here, so we we set the limits for this parameter to [1, 40] in degrees.

# Function fitting

We fit both functions to the averaged reach deviations for each rotation size for each participant in the point target data. The fitting procedure starts with a grid search. Afterwards, we use a bounded optimization method on the top 10 results from the grid search and take the fit with the lowest MSE as the best fit.

We also apply this to the combined data for all participants in each of the two groups.

In the data plot above it looks like the capped function might fit slightly better, let's see how the group-wise fits of the two functions look:

```{r fig.width=5,fig.height=5.5}
fig23_data(models=TRUE)
```

It could still be that the attribution function fits better on individual data, with the peaks of those curves distributed across various rotation sizes, flattening the average data function. So we take the MSE values of the best fits of both function in all 4 subsets of data (the two rotations sizes and the two target types). We can then test which function has the lowest MSE, and hence the best fit to the data set.

## AIC best fit

Here we use the MSEs between the two fits and the averaged reach deviations for each (absolute) rotation to calculate an AIC value for each function fit. These can then be used to calculate a relative likelihood of each model in this data set.

```{r}
modelLikelihood()
```

The capped fixed rate model has the best fit, with the attenuation model much less likely (p<0.001). This means that initial implicit adaptation is not attenuated for larger errors, but rather capped at a maximum level of around 6 degrees.

## Bayesian test

We can also do a paired t-test on the MSEs across all 129 participants. Which should give the same results. Here, we show both a Bayesian and regular t-test:

```{r}
modelTtests()
```

Both the Bayesian t-test and the regular t-test show the same result: the capped model has a better fit.

We don't need this in the paper, as it doesn't really add any information.

## Predicting (implicit) adaptation in long exposure from single-trial learning

Explicit strategies would not benefit participants throughout single-trial learning. This means it might be a better way to assess implicit adaptation than long exposure paradigms where strategies are useful. Participants in our single-trial learning paradigm also engaged in a long exposure block of 200 trials with a 20° rotation that should not evoke any strategies (or at least, not a lot).

We use this to see if STL is related to long exposure adaptation.

Here, we plot A) the learning curves, B) the predictions from capped fixed rate model fitted to the point-target STL data of the first reach deviations in the long exposure curves as captured by the values after 1 trial in an exponential fit. Since the exponentials might be underestimating the first trial, we check if this is the case in C) by comparing the actual first reach deviation with the exponential fits.

```{r fig.width=5.5, fig.height=5.5}
fig4_learningPrediction()
```

First, we can see that participants adapted to the rotation.

The predictions from the capped fixed-rate model seem to overshoot the learning according to the exponential fit. We fit a linear regression, with the intercept set to 0:

```{r}
firstTrialRegression()
```

The fit works. The slope is 0.48175, meaning that STL predicts about twice as much learning as we see in the exponential. Nevertheless, there is a significant relationship (F(1,127)=97.26, p<2.2e-16, R^2_adj=0.4292).

## Potential problems with exponential fit

The slope is lower than 1, which could mean that the exponential fit underestimates the initial learning. There could also be a difference in responses when participants are doing STL or when they do a long exposure task right after an extended aligned baseline with reinforcement of acquiring the target. First, we want to test if the exponential fit's estimate of people's reach deviation after 1 rotated trial is systematically lower than what they actually do.

Let's first look at the data, comparing the exponential fit's values for that first trial with actual behavior on that trial.

The differences between the two don't look very different, but if anything the exponential fit would over-estimate the initial change. Let's test them anyway.

```{r}
firstTrialTest()
```

At least in t-tests, they do not show much of a difference. 

What we can also see is that the raw data is very noisy. The ideal response would have been at 20 degrees, but many reaches go in the opposite direction. So using a function fit on the whole sequence indeed reduces noise, and as the above analysis shows, does not systematically mis-estimate the response on the first trial in either direction. At least not by a large amount. If anything, the average prediction from the exponential fit is a little larger than the average of the actual data, which would indicate an effect in the other direction than what could explain the slopes below 1 in the comparisons between what the STL data says and what the exponential fits say. We'll this rest here for now.

# Test Restest reliability

Here we test if the fits give more or less the same fitted parameters in the first 2 superblocks as they do in the last 2 superblocks (we just leave the middle one be). We plot the second parameter fits over the first parameter fits.

```{r fig.width=6, fig.height=3.5}
fig5_retest()
```

Only the data on the cap parameter looks good.

Let's do linear regressions:

```{r}
doTestRetestRegressions()
```

An ordinary least-squares regression on the cap parameter values is significant [R^2_adj:  0.189, F(1,126)=30.6, p=1.755e-07].

At least within a single session, the cap parameter of the capped fixed-rate function has some measure of reliability. This means that this parameter could be used to compare pre-and post intervention performance within a participant (and within a session) without worrying about savings or even offline gains. Comparing groups on this parameter might also be possible, but could require much more data per participant or many more participants per group to get more accurate fits and account for inter-participant variability.

# Save plots in standard file formats

The chunk below is not executed by default. Run it manually to get all data figures in publication quality formats.

```{r eval=FALSE}
for (target in c('png','pdf','tiff','svg')) {
  fig23_data(target=target)
  fig23_data(models=TRUE, target=target)
  fig4_learningPrediction(target=target)
  fig5_retest(target=target)
}

```


